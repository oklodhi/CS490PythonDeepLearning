{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP7_Question2to4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTl4e7src_jw",
        "colab_type": "text"
      },
      "source": [
        "Make all necessary library imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skzAqWtZao81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import numpy as ns\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import trigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FYx9ldmdC5S",
        "colab_type": "text"
      },
      "source": [
        "Create a Beautiful Soup object and parse the website for data extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqkmWYR1bF6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "website = 'https://en.wikipedia.org/wiki/Google'\n",
        "parser = urlopen(website) \n",
        "soup = BeautifulSoup(parser, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VXqpClxdHS6",
        "colab_type": "text"
      },
      "source": [
        "Create a list to append extracted data into"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL5pVx_KbK9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "somelist = []\n",
        "\n",
        "for link in soup.find_all('p'):\n",
        "  somelist.append(link.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAL1vpyJbPNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(somelist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8f0IrhIdOW_",
        "colab_type": "text"
      },
      "source": [
        "Download NLTK stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEUGgHJIb_lS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "b8106697-e111-4643-da68-6ffef6417f77"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKfO_8M2dQi3",
        "colab_type": "text"
      },
      "source": [
        "Create and open outfut file. Also replace new line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy0_gLQHbvDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('input.txt','w') as filehandle:\n",
        "  for listitem in somelist:\n",
        "    filehandle.write('%s\\n' % listitem)\n",
        "\n",
        "with open('input.txt', 'r') as filehandle:\n",
        "  data = filehandle.read().replace('\\n', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjBZUQ52cGW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUmzqO_hdaSP",
        "colab_type": "text"
      },
      "source": [
        "Applying tokenization to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xap141UcLO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenization = nltk.word_tokenize(data)\n",
        "\n",
        "#print(tokenization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSvgPIWCdfkF",
        "colab_type": "text"
      },
      "source": [
        "Applying POS to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKDfDXSncSWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = nltk.pos_tag(tokenization)\n",
        "\n",
        "#print(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-m0-gkXdhHg",
        "colab_type": "text"
      },
      "source": [
        "Applying Stemming to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXfJFALcev-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemming  = PorterStemmer()\n",
        "stem = '';\n",
        "for w in tokenization:\n",
        "  stem = stem + stemming.stem(w) + ' '\n",
        "\n",
        "#print(stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PULJfwFLdjzR",
        "colab_type": "text"
      },
      "source": [
        "Applying lemmetization to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgcLxKNvcZXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemm = '';\n",
        "for w in tokenization:\n",
        "  lemm = lemm + lemmatizer.lemmatize(w) + ' '\n",
        "\n",
        "#print(lemm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv0c2D9Sdln5",
        "colab_type": "text"
      },
      "source": [
        "Applying trigram to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Sstr1gcmhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trigram = trigrams(tokenization)\n",
        "#for x in trigram:\n",
        "  #print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBTNF_aidnGv",
        "colab_type": "text"
      },
      "source": [
        "Applying named entity recognition to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAIw7YVmcxt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "named_e_r = nltk.ne_chunk(pos, binary=True)\n",
        "\n",
        "#print(named_e_r)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}